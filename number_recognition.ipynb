{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ttight/Desktop/313_Github/linear_algebra/number_recognition.ipynb Cell 2\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ttight/Desktop/313_Github/linear_algebra/number_recognition.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ttight/Desktop/313_Github/linear_algebra/number_recognition.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ttight/Desktop/313_Github/linear_algebra/number_recognition.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(df\u001b[39m.\u001b[39mloc[\u001b[39m1359\u001b[39m][\u001b[39m1\u001b[39m:])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ttight/Desktop/313_Github/linear_algebra/number_recognition.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(image, (\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = np.array(df.loc[1359][1:])\n",
    "image = np.reshape(image, (28, 28))\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things that we are measuring are the images (the overall shape of the points that are in the (x, y) plane or the array above) and the data points/measurements are the pixel locations and values. We are trying to use PCA to measure the picutures with the measurements the pixels.\n",
    "\n",
    "I'm choosing to center the rows of my data around zero. This means that we are essentially measuring the changes in the pixel brightness at the image level. Ie, we are measuring the similarity and differneces of each of the data points along the colums of each image, rather than the rows. This way, we are centering each individual image around the origin, rather than the pixels in each position for each image around the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Centering the columns\n",
    "\n",
    "for i in range(len(df)):\n",
    "  avg = df.loc[i].sum() / len(df.loc[i])\n",
    "  for j in range(1,len(image[0])):\n",
    "    df.loc[i][j] -= avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#getting the covariance matrix\n",
    "\n",
    "covariance_matrix = np.cov(df, rowvar=False)\n",
    "\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "first_principal_component = sorted_eigenvectors[:, 0]\n",
    "\n",
    "image = np.reshape(first_principal_component[1:], (28, 28))\n",
    "plt.imshow(image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "second_principal_component = sorted_eigenvectors[:, 1]\n",
    "image = np.reshape(second_principal_component[1:], (28, 28))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset_df = df.loc[:75, df.columns != \"label\"]\n",
    "labels = df.loc[:75, df.columns == \"label\"][\"label\"]\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(subset_df)\n",
    "\n",
    "\n",
    "transformed_data = pca.transform(subset_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.scatter(transformed_data[:,0], transformed_data[:,1], c=labels)\n",
    "\n",
    "for i, txt in enumerate(labels):\n",
    "  ax.annotate(txt, (transformed_data[:,0][i], transformed_data[:,1][i]))\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 \n",
    "Using only two principal components, we can get closer to an accurate measurement then we would have when guessing, but we cannot get 100% or probably even 90% accuracy with the first two prncipal componenets. we can see above that there are numbers which are more accurate than others, like 1, which is heavily grouped together in the top left hand corner of the graph, but we may not be able to get to high levels of accuracy, where we see in the 0, -600 area there are 4, 5, 7, 8 closely grouped together. Certain more similar digits are grouped together, like the 5's and 2's being grouped closer than the 0's and 4's for example, simply because they look more alike than one another. While the first 2 principal componenets can't accuratly predict these things, we see from the graph above that it certianly makes progress in doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "#function returns the k lengthed array representing the projections onto the k\n",
    "#dimensional space for each image\n",
    "\n",
    "def projections(k):\n",
    "\n",
    "  subset_df = df.loc[:, df.columns != \"label\"]\n",
    "  labels = df.loc[:, df.columns == \"label\"][\"label\"]\n",
    "  pca = PCA(n_components=k)\n",
    "  pca.fit(subset_df)\n",
    "\n",
    "\n",
    "  transformed_data = pca.transform(subset_df)\n",
    "\n",
    "  return transformed_data  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "#Because this calculation is so load heavy on my computer, I'm going to \n",
    "#perform it for only 5000 of the images. \n",
    "\n",
    "from scipy.spatial import distance\n",
    "min_distance = float('inf')\n",
    "#k=10\n",
    "arr = projections(10)\n",
    "labels = df.loc[:, df.columns == \"label\"][\"label\"]\n",
    "\n",
    "for i in range(0, 5000):\n",
    "  for j in range(0, 5000):\n",
    "    if labels[i] != labels[j] and distance.euclidean(arr[i], arr[j]) < min_distance: \n",
    "      min_distance = distance.euclidean(arr[i], arr[j])\n",
    "\n",
    "print(min_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 \n",
    "\n",
    "arr = projections(30)\n",
    "labels = df.loc[:, df.columns == \"label\"][\"label\"]\n",
    "proj_by_numbers = []\n",
    "mean_proj = []\n",
    "for i in range(10):\n",
    "  number_list = [] \n",
    "  for j in range(len(labels)):\n",
    "    if labels[j] == i:\n",
    "      number_list.append(arr[j])\n",
    "  proj_by_numbers.append(np.array(number_list))\n",
    "\n",
    "proj = np.array(proj_by_numbers)\n",
    "for i in range(10):\n",
    "  mean_proj.append(proj_by_numbers[i].mean(axis=0))\n",
    "\n",
    "#here, proj is an array of length 10 with the 10 average points for the numbers\n",
    "#so now we'll use this to figure out which digit my penncard number represents\n",
    "\n",
    "image = arr[1359]\n",
    "min_distance = float('inf')\n",
    "answer = 0\n",
    "for i in range(10):\n",
    "  dist = distance.euclidean(mean_proj[i], image)\n",
    "  if dist<min_distance:\n",
    "    min_distance = dist\n",
    "    answer = i\n",
    "\n",
    "print(labels[1359])\n",
    "print(answer)\n",
    "\n",
    "#for my number above, it thinks that it is closest to the number 9, which it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "#repeating this to get an accuracy representation for all integers\n",
    "\n",
    "predicted_labels = []\n",
    "for i in range(len(arr)):\n",
    "  min_distance = float('inf')\n",
    "  image = arr[i]\n",
    "  answer = 0\n",
    "  for i in range(10):\n",
    "    dist = distance.euclidean(mean_proj[i], image)\n",
    "    if dist < min_distance:\n",
    "      min_distance = dist\n",
    "      answer = i\n",
    "  predicted_labels.append(answer)\n",
    "\n",
    "similar = 0\n",
    "for i in range(len(predicted_labels)):\n",
    "  if predicted_labels[i] == labels[i]:\n",
    "    similar += 1\n",
    "\n",
    "print(similar / len(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 \n",
    "#nearest neighbors projections\n",
    "#because of the runtime of this, I am going to limit the calculation to the first\n",
    "#1000 images\n",
    "import math\n",
    "predicted_labels = []\n",
    "for i in range(1000):\n",
    "  distances = []\n",
    "  for j in range(len(arr)):\n",
    "    dist = []\n",
    "    if i == j:\n",
    "      continue\n",
    "    dist.append(distance.euclidean(arr[i], arr[j]))\n",
    "    dist.append(labels[j])\n",
    "    distances.append(np.array(dist))\n",
    "  distances.sort(key=lambda x: x[0], reverse=False)\n",
    "  distances = np.array(distances)\n",
    "  predicted_labels.append(math.floor(distances[:5].mean(axis =0)[1]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(1000):\n",
    "  if predicted_labels[i] == labels[i]:\n",
    "    correct += 1\n",
    "print(correct / 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the 5 nearest neighbors approach gives us a 90% accuracy, which is much better than the 80% from the previous approach\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#13\n",
    "\n",
    "You should use the maximum amount of principal components to get the maximum accuracy, while this would take a lot of (probably uneccesary) computing power, the maxmium amount of principal componenets would be equal to the rank of the original matrix which is equal to len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
